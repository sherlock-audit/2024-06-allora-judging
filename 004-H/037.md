Dry Sepia Dinosaur

High

# Anyone can override reputers and worker nodes information by using the same LibP2PKey

## Summary
Anyone can override reputers and worker nodes information by using the same `LibP2PKey`.

## Vulnerability Detail
* `LibP2PKey` and `Owner` are user inputs.
* `LibP2PKey` is used as a key to save reputers and worker nodes information.
* There is no validation on the existence of a node with the same `LibP2PKey` when registering a reputer or a worker.
* The `Owner` is used by the Allora node in the reputer losses submission process :
  * `Owner` stakes influence the selection of BlockHeights with the most voting power
  * Duplicate `Owner`s processing is prevented.

All of these combined allow for a situation where anyone can influence the reputer losses submission by either :
* Setting the same `Owner` for multiple `LibP2PKey`s which will lead to bundles coming from all of them but one to be ignored.
* Changing the `Owner`s of multiple `LibP2PKey`s to addresses with either no /  less or more stakes to manipulate the BlockHeights selection.

## Impact
Influence the reputer losses submission and by extension their rewards.

## Code Snippet
[Register](https://github.com/sherlock-audit/2024-06-allora/blob/main/allora-chain/x/emissions/keeper/msgserver/msg_server_registrations.go#L14-L65)
```golang
func (ms msgServer) Register(ctx context.Context, msg *types.MsgRegister) (*types.MsgRegisterResponse, error) {
	if err := msg.Validate(); err != nil {
		return nil, err
	}

	topicExists, err := ms.k.TopicExists(ctx, msg.TopicId)
	if err != nil {
		return nil, err
	}
	if !topicExists {
		return nil, types.ErrTopicDoesNotExist
	}

	hasEnoughBal, fee, err := ms.CheckBalanceForRegistration(ctx, msg.Sender)
	if err != nil {
		return nil, err
	}
	if !hasEnoughBal {
		return nil, types.ErrTopicRegistrantNotEnoughDenom
	}

	// Before creating topic, transfer fee amount from creator to ecosystem bucket
	err = ms.k.SendCoinsFromAccountToModule(ctx, msg.Sender, mintTypes.EcosystemModuleName, sdk.NewCoins(fee))
	if err != nil {
		return nil, err
	}

	nodeInfo := types.OffchainNode{ // <====== Audit
		NodeAddress:  msg.Sender, // <====== Audit
		LibP2PKey:    msg.LibP2PKey, // <====== Audit
		MultiAddress: msg.MultiAddress, // <====== Audit
		Owner:        msg.Owner, // <====== Audit
		NodeId:       msg.Owner + "|" + msg.LibP2PKey, // <====== Audit
	}

	if msg.IsReputer {
		err = ms.k.InsertReputer(ctx, msg.TopicId, msg.Sender, nodeInfo) // <====== Audit
		if err != nil {
			return nil, err
		}
	} else {
		err = ms.k.InsertWorker(ctx, msg.TopicId, msg.Sender, nodeInfo) // <====== Audit
		if err != nil {
			return nil, err
		}
	}

	return &types.MsgRegisterResponse{
		Success: true,
		Message: "Node successfully registered",
	}, nil
}
```

[InsertReputer](https://github.com/sherlock-audit/2024-06-allora/blob/main/allora-chain/x/emissions/keeper/keeper.go#L1432-L1443)
```golang
func (k *Keeper) InsertReputer(ctx context.Context, topicId TopicId, reputer ActorId, reputerInfo types.OffchainNode) error {
	topicKey := collections.Join(topicId, reputer)
	err := k.topicReputers.Set(ctx, topicKey)
	if err != nil {
		return err
	}
	err = k.reputers.Set(ctx, reputerInfo.LibP2PKey, reputerInfo) // <====== Audit
	if err != nil {
		return err
	}
	return nil
}
```

[InsertWorker](https://github.com/sherlock-audit/2024-06-allora/blob/main/allora-chain/x/emissions/keeper/keeper.go#L1462-L1473)
```golang
func (k *Keeper) InsertWorker(ctx context.Context, topicId TopicId, worker ActorId, workerInfo types.OffchainNode) error {
	topickey := collections.Join(topicId, worker)
	err := k.topicWorkers.Set(ctx, topickey)
	if err != nil {
		return err
	}
	err = k.workers.Set(ctx, workerInfo.LibP2PKey, workerInfo) // <====== Audit
	if err != nil {
		return err
	}
	return nil
}
```

[GetWorkerNodeInfo, GetReputerNodeInfo, GetWorkerAddressByP2PKey, GetReputerAddressByP2PKey queries](https://github.com/sherlock-audit/2024-06-allora/blob/main/allora-chain/x/emissions/keeper/queryserver/query_server_registrations.go#L10-L44)
```golang
func (qs queryServer) GetWorkerNodeInfo(ctx context.Context, req *types.QueryWorkerNodeInfoRequest) (*types.QueryWorkerNodeInfoResponse, error) {
	node, err := qs.k.GetWorkerByLibp2pKey(sdk.UnwrapSDKContext(ctx), req.Libp2PKey)  // <====== Audit
	if err != nil {
		return nil, err
	}

	return &types.QueryWorkerNodeInfoResponse{NodeInfo: &node}, nil  // <====== Audit
}

func (qs queryServer) GetReputerNodeInfo(ctx context.Context, req *types.QueryReputerNodeInfoRequest) (*types.QueryReputerNodeInfoResponse, error) {
	node, err := qs.k.GetReputerByLibp2pKey(sdk.UnwrapSDKContext(ctx), req.Libp2PKey)  // <====== Audit
	if err != nil {
		return nil, err
	}

	return &types.QueryReputerNodeInfoResponse{NodeInfo: &node}, nil  // <====== Audit
}

func (qs queryServer) GetWorkerAddressByP2PKey(ctx context.Context, req *types.QueryWorkerAddressByP2PKeyRequest) (*types.QueryWorkerAddressByP2PKeyResponse, error) {
	workerAddr, err := qs.k.GetWorkerAddressByP2PKey(sdk.UnwrapSDKContext(ctx), req.Libp2PKey)  // <====== Audit
	if err != nil {
		return nil, err
	}

	return &types.QueryWorkerAddressByP2PKeyResponse{Address: workerAddr.String()}, nil  // <====== Audit
}

func (qs queryServer) GetReputerAddressByP2PKey(ctx context.Context, req *types.QueryReputerAddressByP2PKeyRequest) (*types.QueryReputerAddressByP2PKeyResponse, error) {
	address, err := qs.k.GetReputerAddressByP2PKey(sdk.UnwrapSDKContext(ctx), req.Libp2PKey)  // <====== Audit
	if err != nil {
		return nil, err
	}

	return &types.QueryReputerAddressByP2PKeyResponse{Address: address.String()}, nil  // <====== Audit
}
```

[GetReputerByLibp2pKey, GetWorkerByLibp2pKey, GetWorkerAddressByP2PKey, GetReputerAddressByP2PKey keepers](https://github.com/sherlock-audit/2024-06-allora/blob/main/allora-chain/x/emissions/keeper/keeper.go#L1485-L1515)
```golang
func (k *Keeper) GetReputerByLibp2pKey(ctx sdk.Context, reputerKey string) (types.OffchainNode, error) {
	return k.reputers.Get(ctx, reputerKey)  // <====== Audit
}

func (k *Keeper) GetWorkerByLibp2pKey(ctx sdk.Context, workerKey string) (types.OffchainNode, error) {
	return k.workers.Get(ctx, workerKey)  // <====== Audit
}

func (k *Keeper) GetWorkerAddressByP2PKey(ctx context.Context, p2pKey string) (sdk.AccAddress, error) {
	worker, err := k.workers.Get(ctx, p2pKey)  // <====== Audit
	if err != nil {
		return nil, err
	}

	workerAddress, err := sdk.AccAddressFromBech32(worker.GetOwner())  // <====== Audit
	if err != nil {
		return nil, err
	}

	return workerAddress, nil  // <====== Audit
}

func (k *Keeper) GetReputerAddressByP2PKey(ctx context.Context, p2pKey string) (sdk.AccAddress, error) {
	reputer, err := k.reputers.Get(ctx, p2pKey)  // <====== Audit
	if err != nil {
		return nil, err
	}

	address, err := sdk.AccAddressFromBech32(reputer.GetOwner())  // <====== Audit
	if err != nil {
		return nil, err
	}

	return address, nil  // <====== Audit
}
```

[SendReputerModeData](https://github.com/sherlock-audit/2024-06-allora/blob/main/allora-inference-base/cmd/node/appchain.go#L530-L655)
```golang
func (ap *AppChain) SendReputerModeData(ctx context.Context, topicId uint64, results aggregate.Results) {
	// Aggregate the forecast from reputer leader
	var valueBundles []*emissionstypes.ReputerValueBundle
	var reputerAddrs []*string // <====== Audit
	var reputerAddrSet = make(map[string]bool) // Prevents duplicate reputer addresses from being counted in vote tally <====== Audit
	var nonceCurrent *emissionstypes.Nonce
	var nonceEval *emissionstypes.Nonce
	var blockCurrentToReputer = make(map[int64][]string) // map blockHeight to addresses of reputers who sent data for current block height <====== Audit
	var blockEvalToReputer = make(map[int64][]string)    // map blockHeight to addresses of reputers who sent data for eval block height <====== Audit

	for _, result := range results {
		if len(result.Peers) > 0 {
			peer := result.Peers[0]
			ap.Logger.Debug().Str("worker peer", peer.String())

			// Get Peer $allo address
			res, err := ap.EmissionsQueryClient.GetReputerAddressByP2PKey(ctx, &emissionstypes.QueryReputerAddressByP2PKeyRequest{
				Libp2PKey: peer.String(),
			}) // <====== Audit
			if err != nil {
				ap.Logger.Warn().Err(err).Str("peer", peer.String()).Msg("error getting reputer peer address from chain, worker not registered? Ignoring peer.")
				continue
			} else {
				// Print the address of the reputer
				ap.Logger.Info().Str("Reputer Address", res.Address).Msg("Reputer Address")
			}

			if _, ok := reputerAddrSet[res.Address]; !ok { // <====== Audit
				reputerAddrSet[res.Address] = true // <====== Audit

				// Parse the result from the reputer to get the losses
				// Parse the result from the worker to get the inferences and forecasts
				var value ReputerDataResponse
				err = json.Unmarshal([]byte(result.Result.Stdout), &value)
				if err != nil {
					ap.Logger.Warn().Err(err).Str("peer", peer.String()).Str("Value", result.Result.Stdout).Msg("error extracting ReputerDataResponse from stdout, ignoring bundle.")
					continue
				}

				// Here reputer leader can choose to validate data further to ensure set is correct and act accordingly
				if value.ReputerValueBundle == nil {
					ap.Logger.Warn().Str("peer", peer.String()).Msg("ReputerValueBundle is nil from stdout, ignoring bundle.")
					continue
				}
				if value.ReputerValueBundle.ValueBundle == nil {
					ap.Logger.Warn().Str("peer", peer.String()).Msg("ValueBundle is nil from stdout, ignoring bundle.")
					continue
				}
				if value.ReputerValueBundle.ValueBundle.TopicId != topicId {
					ap.Logger.Warn().Str("peer", peer.String()).Msg("ReputerValueBundle topicId does not match with request topicId, ignoring bundle.")
					continue
				}
				// Append the WorkerDataBundle (only) to the WorkerDataBundles slice
				valueBundles = append(valueBundles, value.ReputerValueBundle)
				reputerAddrs = append(reputerAddrs, &res.Address) // <====== Audit
				blockCurrentToReputer[value.BlockHeight] = append(blockCurrentToReputer[value.BlockHeight], res.Address) // <====== Audit
				blockEvalToReputer[value.BlockHeightEval] = append(blockEvalToReputer[value.BlockHeightEval], res.Address) // <====== Audit
			}
		} else {
			ap.Logger.Warn().Msg("No peers in the result, ignoring")
		}
	}

	if len(reputerAddrs) == 0 {
		ap.Logger.Warn().Msg("No reputer addresses found, not sending data to the chain")
		return
	}

	blockCurrentHeight, blockEvalHeight, err := ap.getStakeWeightedBlockHeights(ctx, topicId, &blockCurrentToReputer, &blockEvalToReputer, reputerAddrs) // <====== Audit
	if err != nil {
		ap.Logger.Error().Err(err).Msg("could not get stake-weighted block heights, not sending data to the chain")
		return
	}
	if blockCurrentHeight == -1 || blockEvalHeight == -1 {
		ap.Logger.Error().Msg("could not get stake-weighted block heights, not sending data to the chain")
		return
	}
	if blockCurrentHeight < blockEvalHeight {
		ap.Logger.Error().Int64("blockCurrentHeight", blockCurrentHeight).Int64("blockEvalHeight", blockEvalHeight).Msg("blockCurrentHeight < blockEvalHeight, not sending data to the chain")
		return
	}
	nonceCurrent = &emissionstypes.Nonce{BlockHeight: blockCurrentHeight}
	nonceEval = &emissionstypes.Nonce{BlockHeight: blockEvalHeight}

	// Remove those bundles that do not come from the current block height
	var valueBundlesFiltered []*emissionstypes.ReputerValueBundle

	for _, valueBundle := range valueBundles {
		if valueBundle.ValueBundle.ReputerRequestNonce.ReputerNonce.BlockHeight == blockCurrentHeight && valueBundle.ValueBundle.ReputerRequestNonce.WorkerNonce.BlockHeight == blockEvalHeight {  // <====== Audit
			ap.Logger.Debug().
				Str("reputer", valueBundle.ValueBundle.Reputer).
				Str("nonce reputer", strconv.FormatInt(valueBundle.ValueBundle.ReputerRequestNonce.ReputerNonce.BlockHeight, 10)).
				Str("nonce worker", strconv.FormatInt(valueBundle.ValueBundle.ReputerRequestNonce.WorkerNonce.BlockHeight, 10)).
				Msg("Valid nonce, adding to valueBundlesFiltered")
			valueBundlesFiltered = append(valueBundlesFiltered, valueBundle)
		} else {
			ap.Logger.Warn().
				Str("reputer", valueBundle.ValueBundle.Reputer).
				Str("nonce reputer", strconv.FormatInt(valueBundle.ValueBundle.ReputerRequestNonce.ReputerNonce.BlockHeight, 10)).
				Str("nonce worker", strconv.FormatInt(valueBundle.ValueBundle.ReputerRequestNonce.WorkerNonce.BlockHeight, 10)).
				Msg("Rejected Bundle, non-matching nonces.")
		}
	}

	// Make 1 request per worker
	req := &emissionstypes.MsgInsertBulkReputerPayload{
		Sender: ap.Address,
		ReputerRequestNonce: &emissionstypes.ReputerRequestNonce{
			ReputerNonce: nonceCurrent,
			WorkerNonce:  nonceEval,
		},
		TopicId:             topicId,
		ReputerValueBundles: valueBundles,
	}
	// Print req as JSON to the log
	reqJSON, err := json.Marshal(req)
	if err != nil {
		ap.Logger.Error().Err(err).Msg("Error marshaling MsgInsertBulkReputerPayload to print Msg as JSON")
	} else {
		ap.Logger.Info().Str("req_json", string(reqJSON)).Msg("Sending Reputer Mode Data")
	}

	go func() {
		_, _ = ap.SendDataWithRetry(ctx, req, NUM_REPUTER_RETRIES, NUM_REPUTER_RETRY_MIN_DELAY, NUM_REPUTER_RETRY_MAX_DELAY, "Send Reputer Leader Data")
	}()
}
```

[getStakeWeightedBlockHeights](https://github.com/sherlock-audit/2024-06-allora/blob/main/allora-inference-base/cmd/node/appchain.go#L506-L527)
```golang
func (ap *AppChain) getStakeWeightedBlockHeights(
	ctx context.Context,
	topicId uint64,
	blockCurrentToReputer, blockEvalToReputer *map[int64][]string,
	reputerAddrs []*string,
) (int64, int64, error) {
	useWeightedVote := true
	stakesPerReputer, err := ap.getStakePerReputer(ctx, topicId, reputerAddrs) // <====== Audit
	if err != nil {
		ap.Logger.Error().Err(err).Uint64("topic", topicId).Msg("error getting reputer stakes from the chain => using unweighted vote")
		// This removes a strict requirement for the reputer leader to have the correct stake
		// at the cost of potentially allowing sybil attacks, though Blockless itself somewhat mitigates this
		useWeightedVote = false
	}

	// Find the current and ev block height with the highest voting power
	if useWeightedVote {
		return ap.argmaxBlockByStake(blockCurrentToReputer, stakesPerReputer), ap.argmaxBlockByStake(blockEvalToReputer, stakesPerReputer), nil
	} else {
		return ap.argmaxBlockByCount(blockCurrentToReputer), ap.argmaxBlockByCount(blockEvalToReputer), nil
	}
}

func (ap *AppChain) argmaxBlockByStake(
	blockToReputer *map[int64][]string,
	stakesPerReputer map[string]cosmossdk_io_math.Int,
) int64 {
	// Find the current block height with the highest voting power
	firstIter := true
	highestVotingPower := cosmossdk_io_math.ZeroInt()
	blockOfMaxPower := int64(-1)
	for block, reputersWhoVotedForBlock := range *blockToReputer {
		// Calc voting power of this candidate block by total voting reputer stake
		blockVotingPower := cosmossdk_io_math.ZeroInt()
		for _, reputerAddr := range reputersWhoVotedForBlock {
			blockVotingPower = blockVotingPower.Add(stakesPerReputer[reputerAddr])
		}

		// Decide if voting power exceeds that of current front-runner
		if firstIter || blockVotingPower.GT(highestVotingPower) {
			blockOfMaxPower = block
		}

		firstIter = false
	}

	return blockOfMaxPower
}
```

[getStakePerReputer](https://github.com/sherlock-audit/2024-06-allora/blob/main/allora-inference-base/cmd/node/appchain.go#L410-L455)
```golang
func (ap *AppChain) getStakePerReputer(ctx context.Context, topicId uint64, reputerAddrs []*string) (map[string]cosmossdk_io_math.Int, error) {
	maxReputers := DEFAULT_MAX_REPUTERS_FOR_STAKE_QUERY
	params, err := ap.EmissionsQueryClient.Params(ctx, &emissionstypes.QueryParamsRequest{})
	if err != nil {
		ap.Logger.Error().Err(err).Uint64("topic", topicId).Msg("could not get chain params")
	}
	if err == nil {
		maxReputers = params.Params.MaxPageLimit
	}

	numberRequestsForStake := MAX_NUMBER_STAKE_QUERIES_PER_REQUEST
	var stakesPerReputer = make(map[string]cosmossdk_io_math.Int) // This will be populated with each request/loop below
	for i := uint64(0); i < numberRequestsForStake; i++ {
		// Dereference only the needed reputer addresses to get the actual strings
		addresses := make([]string, 0)
		start := i * maxReputers
		end := (i + 1) * maxReputers
		if end > uint64(len(reputerAddrs)) {
			end = uint64(len(reputerAddrs))
		}
		if start >= end {
			break
		}
		for _, addr := range reputerAddrs[start:end] {
			if addr == nil {
				return nil, fmt.Errorf("nil address in reputerAddrs")
			}
			addresses = append(addresses, *addr)
		}
		res, err := ap.EmissionsQueryClient.GetMultiReputerStakeInTopic(ctx,  &emissionstypes.QueryMultiReputerStakeInTopicRequest{  // <====== Audit
			TopicId:   topicId,
			Addresses: addresses,  // <====== Audit
		})
		if err != nil {
			ap.Logger.Error().Err(err).Uint64("topic", topicId).Msg("could not get reputer stakes from the chain")
			return nil, err
		}

		// Create a map of reputer addresses to their stakes
		for _, stake := range res.Amounts {
			stakesPerReputer[stake.Reputer] = stake.Amount
		}
	}

	return stakesPerReputer, err
}
```

## Tool used
Manual Review

## Recommendation
Check if a reputer / worker node information  with the same `LibP2PKey` exists and validate that the `NodeAddress` is the same  as the `msg.Sender`.